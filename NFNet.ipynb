{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers \n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfnet_params = {'F0': {'width': [256, 512, 1536, 1536], 'depth': [1, 2, 6, 3],'drop_rate': 0.2},\n",
    "                'F1': {'width': [256, 512, 1536, 1536], 'depth': [2, 4, 12, 6],'drop_rate': 0.3},\n",
    "                'F2': {'width': [256, 512, 1536, 1536], 'depth': [3, 6, 18, 9],'drop_rate': 0.4},\n",
    "                'F3': {'width': [256, 512, 1536, 1536], 'depth': [4, 8, 24, 12],'drop_rate': 0.4},\n",
    "                'F4': {'width': [256, 512, 1536, 1536], 'depth': [5, 10, 30, 15],'drop_rate': 0.5},\n",
    "                'F5': {'width': [256, 512, 1536, 1536], 'depth': [6, 12, 36, 18],'drop_rate': 0.5},\n",
    "                'F6': {'width': [256, 512, 1536, 1536], 'depth': [7, 14, 42, 21],'drop_rate': 0.5},\n",
    "                'F7': {'width': [256, 512, 1536, 1536], 'depth': [8, 16, 48, 24], 'drop_rate': 0.5}}\n",
    "\n",
    "nonlinearities = {\"identity\": lambda x: x,\n",
    "                  \"celu\": lambda x: tf.nn.crelu(x) * 1.270926833152771,\n",
    "                  \"elu\": lambda x: tf.nn.elu(x) * 12716004848480225,\n",
    "                  \"gelu\": lambda x: tf.nn.gelu(x) * 1.7015043497085571,\n",
    "                  \"leaky_relu\": lambda x: tf.nn.leaky_relu(x) * 1.70590341091156,\n",
    "                  \"log_sigmoid\": lambda x: tf.math.log(tf.nn.sigmoid(x)) * 1.9193484783172607,\n",
    "                  \"log_softmax\": lambda x: tf.math.log(tf.nn.softmax(x)) * 1.0002083778381348,\n",
    "                  \"relu\": lambda x: tf.nn.relu(x) * 1.7139588594436646,\n",
    "                  \"relu6\": lambda x: tf.nn.relu6(x) * 1.7131484746932983,\n",
    "                  \"selu\": lambda x: tf.nn.selu(x) * 1.0008515119552612,\n",
    "                  \"sigmoid\": lambda x: tf.nn.sigmoid(x) * 4.803835391998291,\n",
    "                  \"silu\": lambda x: tf.nn.silu(x) * 1.7881293296813965,\n",
    "                  \"soft_sign\": lambda x: tf.nn.softsign(x) * 2.338853120803833,\n",
    "                  \"softplus\": lambda x: tf.nn.softplus(x) * 1.9203323125839233,\n",
    "                  \"tanh\": lambda x: tf.nn.tanh(x) * 1.5939117670059204,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2D(layers.Conv2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(WSConv2D, self).__init__(kernel_initializer=keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='untruncated_normal',), *args, **kwargs)\n",
    "        self.gain = self.add_weight(name='gain',shape=(self.filters,),initializer=\"ones\",trainable=True,dtype=self.dtype)\n",
    "\n",
    "    def standardize_weight(self, eps):\n",
    "        mean = tf.math.reduce_mean(self.kernel, axis=(0, 1, 2), keepdims=True)\n",
    "        var = tf.math.reduce_variance(self.kernel, axis=(0, 1, 2), keepdims=True)\n",
    "        fan_in = tf.cast(tf.math.reduce_prod(self.kernel.shape[:-1]),'float32')\n",
    "        scale = tf.math.rsqrt(tf.math.maximum(var * fan_in,eps)) * self.gain\n",
    "        shift = mean * scale\n",
    "        return self.kernel * scale - shift\n",
    "\n",
    "    def call(self, inputs, eps=1e-4):\n",
    "        weight = self.standardize_weight(eps)\n",
    "        return tf.nn.conv2d(inputs, weight, strides=self.strides, padding=self.padding.upper(), dilations=self.dilation_rate) + self.bias\n",
    "    \n",
    "class SE_Block(keras.Model):\n",
    "    def __init__(self,out_ch,se_ratio=0.5, act='relu'):\n",
    "        super(SE_Block, self).__init__()\n",
    "        self.GAP = layers.GlobalAvgPool2D()\n",
    "        self.main = keras.Sequential([layers.Conv2D(int(out_ch*se_ratio),1,activation = act),layers.Conv2D(out_ch,1)])\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.GAP(x)[:,tf.newaxis,tf.newaxis]\n",
    "        x = self.main((x))\n",
    "        return tf.nn.sigmoid(x) * x\n",
    "\n",
    "class StochDepth(keras.Model):\n",
    "    def __init__(self, drop_rate):\n",
    "        super(StochDepth, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def call(self, x, training):\n",
    "        if not training:\n",
    "            return x\n",
    "        r = tf.random.uniform(shape=[x.shape[0], 1, 1, 1], dtype=x.dtype)\n",
    "        keep_prob = 1. - self.drop_rate\n",
    "        binary_tensor = tf.floor(keep_prob + r)\n",
    "        return x * binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFBlock(keras.Model):\n",
    "    def __init__(self,in_ch,out_ch,expansion=0.5,se_ratio=0.5,kernel_shape=3,group_size=128,stride=1,\n",
    "                 beta=1.0,alpha=0.2,activation=None,use_two_convs=True,stochdepth_rate=None):\n",
    "        super(NFBlock, self).__init__()\n",
    "        self.beta, self.alpha = beta, alpha\n",
    "        self.activation = activation\n",
    "        self.avgpool = layers.AveragePooling2D(int(stride)) if stride > 1 else None\n",
    "        self.conv0 = WSConv2D(group_size * int((out_ch) * expansion) // group_size,1,1,padding=\"same\")\n",
    "        self.conv1 = WSConv2D(group_size * int((out_ch) * expansion) // group_size,kernel_shape,int(stride),padding=\"same\",groups=int((out_ch) * expansion) // group_size)\n",
    "        self.conv1b = WSConv2D(group_size * int((out_ch) * expansion) // group_size,kernel_shape,1,padding=\"same\",groups=int((out_ch) * expansion) // group_size) if use_two_convs else None\n",
    "        self.conv2 = WSConv2D(out_ch,1,1,padding=\"same\")\n",
    "        self.short_conv = WSConv2D(out_ch,1,padding=\"same\") if stride>1 or in_ch != out_ch else None\n",
    "        self.se = SE_Block(out_ch,se_ratio)\n",
    "        self.stoch_depth = StochDepth(stochdepth_rate)  if (stochdepth_rate is not None) else None\n",
    "        self.skip_gain = self.add_weight(name=\"skip_gain\",shape=(),initializer=\"zeros\",trainable=True,dtype=self.dtype)\n",
    "            \n",
    "    def call(self, x, training):\n",
    "        out = layers.Lambda(self.activation)(x * self.beta)\n",
    "        x = self.avgpool(out) if self.avgpool else x\n",
    "        shortcut = self.short_conv(x) if self.short_conv  else x\n",
    "        out = self.conv0(out)\n",
    "        out = self.conv1(layers.Lambda(self.activation)(out))\n",
    "        out = self.conv1b(layers.Lambda(self.activation)(out)) if self.conv1b else out\n",
    "        out = self.conv2(layers.Lambda(self.activation)(out))\n",
    "        out = self.se(out) * 2.0\n",
    "        out = self.stoch_depth(out, training) if self.stoch_depth else out\n",
    "        out = out * self.skip_gain\n",
    "        return out * self.alpha + shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_NFNet(num_classes=1000,variant=\"F0\",group=128,expand_width = 1.0,se_ratio=0.5,alpha=0.2,depth_rate=0.1,act=\"relu\",last_expand=2):\n",
    "    activation = nonlinearities[act]\n",
    "    Input_x = layers.Input((224,224,3))\n",
    "    x = WSConv2D(16,3,2,padding=\"same\")(Input_x)\n",
    "    x = layers.Lambda(activation)(x)\n",
    "    x = WSConv2D(32,3,1,padding=\"same\")(x)\n",
    "    x = layers.Lambda(activation)(x)\n",
    "    x = WSConv2D(64,3,1,padding=\"same\")(x)\n",
    "    x = layers.Lambda(activation)(x)\n",
    "    x = WSConv2D(nfnet_params[variant]['width'][0]//2,3,2,padding=\"same\")(x)\n",
    "    index = 0  \n",
    "    for (block_width,stage_depth,stride) in zip(nfnet_params[variant]['width'],nfnet_params[variant]['depth'],[1]+[2]*len(nfnet_params[variant]['depth'])):\n",
    "        expected_std = 1.0\n",
    "        for block_index in range(stage_depth):\n",
    "            x = NFBlock(x.shape[3],(block_width*expand_width),0.5,se_ratio,3,128,stride if block_index == 0 else 1,\n",
    "                        1.0/expected_std,alpha,activation,True,depth_rate*index/sum(nfnet_params[variant]['depth']))(x)\n",
    "            expected_std = (expected_std **2 + alpha**2)**0.5\n",
    "            index += 1\n",
    "    x = WSConv2D(int(last_expand * x.shape[3]),1, padding=\"same\")(x)\n",
    "    x = layers.Lambda(activation)(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(nfnet_params[variant]['drop_rate'])(x)\n",
    "    x = layers.Dense(num_classes)(x)\n",
    "    return keras.Model(Input_x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def clip_gradient(grad, weight, clipping=0.01, eps=1e-3):\n",
    "    param_norm = tf.math.maximum(tf.math.reduce_sum(weight**2, axis=[0,1,2], keepdims=True) ** 0.5, eps)\n",
    "    grad_norm = tf.math.reduce_sum(grad**2, axis=[0,1,2], keepdims=True)\n",
    "    max_norm = param_norm * clipping\n",
    "    trigger = grad_norm < max_norm\n",
    "    clipped_grad = grad * (max_norm / tf.maximum(grad_norm, 1e-6))\n",
    "    return tf.where(trigger, grad, clipped_grad)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def unitwise_norm(x):\n",
    "    if len(x.shape) <= 1:\n",
    "        axis = None\n",
    "        keepdims = False\n",
    "    elif len(x.shape) in [2, 3]:\n",
    "        axis = 0\n",
    "        keepdims = True\n",
    "    elif len(x.shape) == 4:\n",
    "        axis = [0, 1, 2]\n",
    "        keepdims = True\n",
    "    return tf.math.reduce_sum(x ** 2, axis=axis, keepdims=keepdims) ** 0.5\n",
    "\n",
    "@tf.function\n",
    "def clip_gradient(grad, weight, clipping=0.01, eps=1e-3):\n",
    "    param_norm = tf.math.maximum(unitwise_norm(weight), eps)\n",
    "    grad_norm = unitwise_norm(grad)\n",
    "    max_norm = param_norm * clipping\n",
    "    trigger = grad_norm < max_norm\n",
    "    clipped_grad = grad * (max_norm / tf.math.maximum(grad_norm, 1e-6))\n",
    "    return tf.where(trigger, grad, clipped_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y,model,optimizer,ema,clipping_factor=0.01):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y, y_pred)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    clipped_gradients = []\n",
    "    for grad, weight in zip(gradients, model.trainable_weights):\n",
    "        if (\"dense\" in weight.name):\n",
    "            clipped_gradients.append(grad)\n",
    "        else:\n",
    "            clipped_gradients.append(clip_gradient(grad, weight, clipping=clipping_factor))\n",
    "    decay_var_list = []\n",
    "    for layer in model.trainable_weights:\n",
    "        if not (\"gain\" in layer.name or \"bias\" in layer.name):\n",
    "            decay_var_list.append(layer)\n",
    "    opt_op = optimizer.apply_gradients(zip(clipped_gradients, model.trainable_weights),decay_var_list=decay_var_list)\n",
    "    with tf.control_dependencies([opt_op]):\n",
    "        ema.apply(model.trainable_variables)    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUp(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,initial_learning_rate,decay_schedule_fn,warmup_steps,power = 1.0):\n",
    "        super().__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.power = power\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "\n",
    "    def __call__(self, step):\n",
    "        global_step_float = tf.cast(step, tf.float32)\n",
    "        warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_percent_done = global_step_float / warmup_steps_float\n",
    "        warmup_learning_rate = self.initial_learning_rate * tf.math.pow(warmup_percent_done, self.power)\n",
    "        return tf.cond(global_step_float < warmup_steps_float,lambda: warmup_learning_rate,lambda: self.decay_schedule_fn(step - self.warmup_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py:458: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "tf.Tensor(0.7018745, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70016325, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70016336, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7004414, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69982904, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6995492, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7006166, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69781744, shape=(), dtype=float32)\n",
      "tf.Tensor(0.703122, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7002796, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7002015, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6988789, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70240563, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70096356, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70213974, shape=(), dtype=float32)\n",
      "tf.Tensor(0.703694, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70119774, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7011466, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6993677, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70301676, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7009181, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6999949, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70137286, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70299804, shape=(), dtype=float32)\n",
      "tf.Tensor(0.701663, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6998672, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69898665, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6997194, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70080066, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7017559, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6994972, shape=(), dtype=float32)\n",
      "tf.Tensor(0.701449, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7034009, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7025231, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7005997, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70053935, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6997256, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7008663, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69891024, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70147574, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70098555, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70029014, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69888926, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7003129, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6992813, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6988412, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6988584, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6997622, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6986729, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7025368, shape=(), dtype=float32)\n",
      "tf.Tensor(0.701656, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6988926, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69959086, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7005019, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7033559, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70118296, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69969726, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69864166, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70111537, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69897205, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69970435, shape=(), dtype=float32)\n",
      "tf.Tensor(0.701613, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7003993, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70212907, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70039016, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69953763, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7009568, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70031327, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69878006, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70213985, shape=(), dtype=float32)\n",
      "tf.Tensor(0.698836, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7005048, shape=(), dtype=float32)\n",
      "tf.Tensor(0.698401, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7006857, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69958425, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69920826, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7009143, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7015427, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70129263, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7017617, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7035934, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7007744, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7002452, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70087415, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69906765, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69856465, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7008902, shape=(), dtype=float32)\n",
      "tf.Tensor(0.70054054, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6986885, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6994074, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6984663, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6983664, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6983382, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69937015, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6997026, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7016916, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6991108, shape=(), dtype=float32)\n",
      "tf.Tensor(0.69901556, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6991973, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6999457, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "variant=\"F0\"\n",
    "num_step=10\n",
    "warm_step = 5\n",
    "lr=0.1\n",
    "clipping=0.01\n",
    "batch_size = 4\n",
    "max_lr = lr * batch_size / 256\n",
    "ema_decay = 0.99999\n",
    "\n",
    "lr_decayed_fn = keras.experimental.CosineDecay(max_lr,num_step - 5000)\n",
    "lr_schedule = WarmUp(max_lr,lr_decayed_fn,5000)\n",
    "ema = tf.train.ExponentialMovingAverage(decay=ema_decay)\n",
    "\n",
    "model = make_NFNet(NUM_CLASSES,variant)\n",
    "optimizer = tfa.optimizers.SGDW(learning_rate=lr_schedule, weight_decay=2e-5, momentum=0.9)\n",
    "\n",
    "x = np.concatenate([np.ones((2,224,224,3),'float32'),np.zeros((2,224,224,3),'float32')])\n",
    "y = np.concatenate([np.ones((2,1),'float32'),np.zeros((2,1),'float32')])\n",
    "\n",
    "for i in range(100):\n",
    "    print(train_step(x,y,model,optimizer,ema,clipping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
